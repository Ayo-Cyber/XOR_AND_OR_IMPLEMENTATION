{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Oh9SuqadFJc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    \"\"\"\n",
        "    Attributes:\n",
        "        layers (list of int): Number of neurons per layer, e.g. [2, 2, 1].\n",
        "        alpha (float): Learning rate for gradient descent.\n",
        "        W (list of np.ndarray): Weight matrices for each layer transition.\n",
        "        b (list of np.ndarray): Bias vectors for each layer transition.\n",
        "    \"\"\"\n",
        "    def __init__(self, layers, alpha=0.1):\n",
        "        self.layers = layers\n",
        "        self.alpha = alpha\n",
        "        self.W = []  # weights: list of matrices of shape (n_in, n_out)\n",
        "        self.b = []  # biases: list of row-vectors of shape (1, n_out)\n",
        "\n",
        "        for i in range(len(layers) - 1):\n",
        "            n_in, n_out = layers[i], layers[i+1]\n",
        "            # Xavier initialization: keeps variance of activations stable\n",
        "            W = np.random.randn(n_in, n_out) / np.sqrt(n_in)\n",
        "            b = np.zeros((1, n_out))\n",
        "            self.W.append(W)\n",
        "            self.b.append(b)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"NeuralNetwork: {}\".format(\"-\".join(map(str, self.layers)))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_deriv(self, sigmoid_x):\n",
        "        return sigmoid_x * (1.0 - sigmoid_x)\n",
        "\n",
        "    def fit(self, X, y, epochs=1000, display_update=100):\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            for x_i, y_i in zip(X, y):\n",
        "                self._update_sample(x_i, y_i)\n",
        "            if epoch == 1 or epoch % display_update == 0:\n",
        "                loss = self.calculate_loss(X, y)\n",
        "                print(f\"[INFO] epoch={epoch}, loss={loss:.7f}\")\n",
        "\n",
        "    def _update_sample(self, x, target):\n",
        "        # --- FORWARD PASS ---\n",
        "        activations = [np.atleast_2d(x)]\n",
        "        for W, b in zip(self.W, self.b):\n",
        "            z = activations[-1] @ W + b\n",
        "            a = self.sigmoid(z)\n",
        "            activations.append(a)\n",
        "\n",
        "        # --- BACKPROPAGATION ---\n",
        "        # Output-layer error and delta\n",
        "        output = activations[-1]\n",
        "        error = output - target\n",
        "        deltas = [error * self.sigmoid_deriv(output)]\n",
        "\n",
        "        # Hidden-layer deltas\n",
        "        for i in range(len(self.W) - 1, 0, -1):\n",
        "            delta_next = deltas[-1]\n",
        "            z_hidden = activations[i]\n",
        "            delta = (delta_next @ self.W[i].T) * self.sigmoid_deriv(z_hidden)\n",
        "            deltas.append(delta)\n",
        "        deltas.reverse()\n",
        "\n",
        "        # --- WEIGHT & BIAS UPDATES ---\n",
        "        for i, (W, b) in enumerate(zip(self.W, self.b)):\n",
        "            a_prev = activations[i]\n",
        "            delta = deltas[i]\n",
        "            grad_W = a_prev.T @ delta\n",
        "            grad_b = delta\n",
        "            self.W[i] = W - self.alpha * grad_W\n",
        "            self.b[i] = b - self.alpha * grad_b\n",
        "\n",
        "    def predict(self, X):\n",
        "        a = np.atleast_2d(X)\n",
        "        for W, b in zip(self.W, self.b):\n",
        "            a = self.sigmoid(a @ W + b)\n",
        "        return a\n",
        "\n",
        "    def calculate_loss(self, X, targets):\n",
        "        predictions = self.predict(X)\n",
        "        errors = predictions - targets\n",
        "        return 0.5 * np.sum(errors**2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Truth table inputs\n",
        "    X = np.array([[0,0],\n",
        "                  [0,1],\n",
        "                  [1,0],\n",
        "                  [1,1]])\n",
        "\n",
        "    # Correct outputs for each gate\n",
        "    y_and = np.array([[0],[0],[0],[1]])\n",
        "    y_or  = np.array([[0],[1],[1],[1]])\n",
        "    y_xor = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "    # AND\n",
        "    print(\"=== AND ===\")\n",
        "    print(\"Inputs:\\n\", X)\n",
        "    print(\"Correct AND outputs:\\n\", y_and.flatten())\n",
        "    nn_and = NeuralNetwork([2,1], alpha=0.5)\n",
        "    nn_and.fit(X, y_and, epochs=1000, display_update=500)\n",
        "    print(\"Predicted AND:\\n\", nn_and.predict(X).round().astype(int).flatten(), \"\\n\")\n",
        "\n",
        "    # OR\n",
        "    print(\"=== OR ===\")\n",
        "    print(\"Inputs:\\n\", X)\n",
        "    print(\"Correct OR outputs:\\n\", y_or.flatten())\n",
        "    nn_or = NeuralNetwork([2,1], alpha=0.5)\n",
        "    nn_or.fit(X, y_or, epochs=1000, display_update=500)\n",
        "    print(\"Predicted OR:\\n\", nn_or.predict(X).round().astype(int).flatten(), \"\\n\")\n",
        "\n",
        "    # XOR\n",
        "    print(\"=== XOR ===\")\n",
        "    print(\"Inputs:\\n\", X)\n",
        "    print(\"Correct XOR outputs:\\n\", y_xor.flatten())\n",
        "    nn_xor = NeuralNetwork([2,2,1], alpha=0.5)\n",
        "    nn_xor.fit(X, y_xor, epochs=5000, display_update=1000)\n",
        "    print(\"Predicted XOR:\\n\", nn_xor.predict(X).round().astype(int).flatten())\n"
      ],
      "metadata": {
        "id": "GFpv8XvinqNV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09bc5594-b7fb-43b8-aaf5-c6ce37c5f660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== AND ===\n",
            "Inputs:\n",
            " [[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 1]]\n",
            "Correct AND outputs:\n",
            " [0 0 0 1]\n",
            "[INFO] epoch=1, loss=0.4999198\n",
            "[INFO] epoch=500, loss=0.0238759\n",
            "[INFO] epoch=1000, loss=0.0111558\n",
            "Predicted AND:\n",
            " [0 0 0 1] \n",
            "\n",
            "=== OR ===\n",
            "Inputs:\n",
            " [[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 1]]\n",
            "Correct OR outputs:\n",
            " [0 1 1 1]\n",
            "[INFO] epoch=1, loss=0.6764941\n",
            "[INFO] epoch=500, loss=0.0129715\n",
            "[INFO] epoch=1000, loss=0.0058418\n",
            "Predicted OR:\n",
            " [0 1 1 1] \n",
            "\n",
            "=== XOR ===\n",
            "Inputs:\n",
            " [[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 1]]\n",
            "Correct XOR outputs:\n",
            " [0 1 1 0]\n",
            "[INFO] epoch=1, loss=0.6262052\n",
            "[INFO] epoch=1000, loss=0.0676765\n",
            "[INFO] epoch=2000, loss=0.0057782\n",
            "[INFO] epoch=3000, loss=0.0028275\n",
            "[INFO] epoch=4000, loss=0.0018475\n",
            "[INFO] epoch=5000, loss=0.0013647\n",
            "Predicted XOR:\n",
            " [0 1 1 0]\n"
          ]
        }
      ]
    }
  ]
}